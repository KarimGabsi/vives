---
sidebar_position: 1
---

# Official Tutorial + Exercise

:::note
Documentation: [docs](https://docs.scrapy.org/en/latest/) <br />
Official tutorial: [official tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) <br />
Quotes-bot: [github](https://github.com/scrapy/quotesbot) <br />
:::

## Installation

1) Open pycharm and start a new project with a virtual environment. <br />
2) Install `Scrapy` <br />
3) Find *scrapy.exe* in your scripts folder <br />
Example: C:\Users\{username}\PycharmProjects\Scrapy\venv\Scripts\\**Scrapy.exe**
:::tip

In pycharm you can right-click on a file and select **Copy Path/Reference...**

:::

4) Copy the path and go to your terminal **within** the same project  <br />
5) Execute the command:

```text
C:\Users\{username}\PycharmProjects\Scrapy\venv\Scripts\Scrapy.exe startproject tutorial
```

By now you should have a tutorial folder in your project with the following layout:

```text
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

## Creating a Spider

:::danger

When making a Spider, don't forget to inherit from the Spider-Class

:::

In `tutorial/spiders` create a quotes_spider.py file:

```python title="quotes_spider.py"
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'https://quotes.toscrape.com/page/1/',
            'https://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = f'quotes-{page}.html'
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log(f'Saved file {filename}')
```

## Running the Spider

### Method from the official tutorial page

Go to your project terminal and execute the following line:
```text 
C:\Users\{username}\PycharmProjects\Scrapy\venv\Scripts\Scrapy.exe crawl quotes
```

### The Python way!

:::tip
Scrapy from scripts: [docs](https://docs.scrapy.org/en/latest/topics/practices.html)
:::

1) open `main.py` from your project (delete everything if you have the template version)
```python
if __name__ == '__main__':
    pass
```
2) import the `CrawlerProcess` & your `QuotesSpider`-class
```python
from tutorial.tutorial.spiders.quotes_spider import QuotesSpider
from scrapy.crawler import CrawlerProcess
```

3) Use the crawler:
```python
from tutorial.tutorial.spiders.quotes_spider import QuotesSpider
from scrapy.crawler import CrawlerProcess

if __name__ == '__main__':
    cp = CrawlerProcess()
    cp.crawl(QuotesSpider)
    cp.start()

```

:::warning
Running this will give you an error. <br />
Scrapy needs a configuration for to set the crawler settings.
:::

4) import our our project settings

```python
from scrapy.utils.project import get_project_settings
```

5) modify our code to use our project settings and run!

```python 
from tutorial.tutorial.spiders.quotes_spider import QuotesSpider
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

if __name__ == '__main__':
    config = get_project_settings()
    cp = CrawlerProcess(settings=config)
    cp.crawl(QuotesSpider)
    cp.start()
```

## Exercise

Create a new quotes-spider class and name it **quotesclassified**.
It should be able to do the following:

- Modify your class to be able to receive a variable in order to scrape an n-amount of pages. <br />
  Set n = 0 as a default. If the default is enabled, scrape all the pages possible.
- The example saves the extracted data as html file, so modify your code in order to do the following:
  - Scrape quotes with their according tags.
  - Save the extracted data in a CSV file.